<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title><?php echo $pageTitle; ?></title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <nav>
                <ul>
                    <li><a href="index.php">Accueil</a></li>
                    <li><a href="projects.php">Projets</a></li>
                </ul>
            </nav>
        </header>

<main>
    <h1>Mes Projets</h1>

    <div class="projects-tabs">
        <button type="button" class="tab active" data-project="inondations">
            üåä Inondations
        </button>
        <button type="button" class="tab" data-project="lidar">
            üü¢ LiDAR HD
        </button>
        <button type="button" class="tab" data-project="segmentation">
            üß† Segmentation IA
        </button>
        <button type="button" class="tab" data-project="sig">
            üó∫Ô∏è Analyse spatiale
        </button>
        <button type="button" class="tab" data-project="ndwi">
            üíß D√©tection de l'eau
        </button>
    </div>

    <section class="project active" id="project-inondations">
        <h2>Approche de d√©tection automatique pour la d√©limitation des zones inondables canadiennes</h2>

        <figure>
            <img src="images/inondations/pres.jpg" alt="Exemple d'inondation" class="project-image">
            <figcaption>Exemple d'inondation en zone urbaine.</figcaption>
        </figure>

        <h3>Contexte</h3>
        <p>
            Les inondations font partie des catastrophes naturelles les plus fr√©quentes et destructrices au Canada.
            Ce projet, r√©alis√© √† l'INRS (Institut National de la Recherche Scientifique) au sein du laboratoire TENOR,
            vise √† d√©velopper une approche automatique pour la d√©tection des zones inondables en utilisant des images satellites.
        </p>

        <h3>Objectifs</h3>
        <p>
            L'objectif principal est de d√©velopper un mod√®le de deep learning capable de d√©tecter les zones inond√©es √† partir d'images Sentinel-2.
            Le mod√®le doit √™tre capable de distinguer les zones d'eau, de nuages, et de sol pour fournir des cartes pr√©cises des inondations.
        </p>

        <h3>M√©thodologie</h3>
        <p>
            <strong>Pr√©traitement des donn√©es :</strong>
            <ul>
                <li>Transformation des images Sentinel-2 en pseudo-Landsat pour une meilleure compatibilit√© avec les donn√©es historiques.</li>
                <li>R√©duction de la r√©solution des images de 10m √† 30m.</li>
                <li>Cr√©ation de vecteurs d'entr√©e pour chaque pixel en utilisant les valeurs de r√©flectance.</li>
            </ul>
        </p>

        <p>
            <strong>Mod√®le de Deep Learning :</strong>
            <ul>
                <li>Utilisation d'un r√©seau de neurones convolutif (CNN) pour la segmentation s√©mantique.</li>
                <li>Architecture du mod√®le : couches de convolution, de pooling, et couches denses.</li>
                <li>Optimisation du mod√®le avec des techniques de r√©gularisation (L1, L2) et de pond√©ration des classes.</li>
            </ul>
        </p>

        <figure>
            <img src="images/inondations/orga.png" alt="Organigramme du projet" class="project-image" style="width: 70%; height: auto; margin: 20px auto;">
            <figcaption>Organigramme du projet de d√©tection des inondations.</figcaption>
        </figure>

        <h3>R√©sultats</h3>
        <p>
            <strong>D√©tection des nuages :</strong>
            <ul>
                <li>Pr√©cision de 99% sur les donn√©es de calibration.</li>
                <li>Am√©lioration de la g√©n√©ralisation du mod√®le avec des techniques de r√©gularisation.</li>
                <li>Utilisation d'un filtre modal pour r√©duire les erreurs de surestimation des nuages.</li>
            </ul>
        </p>

        <div class="side-by-side-images">
            <div class="side-image">
                <figure>
                    <img src="images/inondations/mat1.png" alt="Matrice de confusion pour la d√©tection des nuages" class="project-image">
                    <figcaption>Matrice de confusion pour la d√©tection des nuages (jeu d'entrainement).</figcaption>
                </figure>
            </div>
            <div class="side-image">
                <figure>
                    <img src="images/inondations/mat2.png" alt="Matrice de confusion pour la d√©tection des nuages" class="project-image">
                    <figcaption>Matrice de confusion pour la d√©tection des nuages (jeu de validation).</figcaption>
                </figure>
            </div>
        </div>

        <p>
            <strong>D√©tection de l'eau :</strong>
            <ul>
                <li>Pr√©cision de 96% sur les donn√©es de calibration.</li>
                <li>Utilisation des pr√©dictions des nuages pour am√©liorer la d√©tection de l'eau.</li>
            </ul>
        </p>

        <figure>
            <img src="images/inondations/graphe.png" alt="Graphique de pr√©cision et perte" class="project-image">
            <figcaption>Graphique de pr√©cision et de perte lors de l'entra√Ænement du mod√®le.</figcaption>
        </figure>

        <h3>Conclusion</h3>
        <p>
            Ce projet a permis de d√©velopper un mod√®le de deep learning efficace pour la d√©tection des zones inondables.
            Les r√©sultats montrent une bonne pr√©cision et une capacit√© √† g√©n√©raliser sur de nouvelles donn√©es.
            Des am√©liorations futures pourraient inclure l'analyse fr√©quentielle pour ajouter une dimension temporelle aux pr√©dictions.
        </p>

        <div class="project-skills">
            <h3>Comp√©tences mobilis√©es</h3>
            <ul>
                <li>Traitement d'images satellites</li>
                <li>Deep Learning</li>
                <li>Segmentation s√©mantique</li>
                <li>CNN</li>
                <li>Python</li>
                <li>TensorFlow/Keras</li>
                <li>QGIS</li>
                <li>Pr√©traitement des donn√©es</li>
                <li>Optimisation des mod√®les</li>
                <li>Visualisation des r√©sultats</li>
            </ul>
        </div>
    </section>






    <section class="project" id="project-lidar">
        <h2>Qualification de la classification LIDAR HD</h2>

        <p class="project-context">
            Ce travail, r√©alis√© au sein du service SV3D de l‚ÄôIGN, vise √† r√©pondre √† une question
            simple mais cruciale : comment mesurer et automatiser l‚Äô√©valuation de la qualit√© de la
            classification des nuages de points LiDAR HD ? Alors que les caract√©ristiques
            g√©om√©triques (position, altitude) des dalles sont contr√¥l√©es, la composante s√©mantique,
            c‚Äôest-√†-dire savoir si un point est correctement √©tiquet√© en ¬´ sol ¬ª, ¬´ b√¢ti ¬ª,
            ¬´v√©g√©tation¬ª, etc‚Ä¶ n‚Äôest pas quantifi√©e √† grande √©chelle.
        </p>

        <figure>
            <img src="images/lidar/nuage_lidar.png" alt="Exemple d‚Äôun nuage de points Lidar HD en zone urbaine" class="project-image">
            <figcaption>Exemple d‚Äôun nuage de points LiDAR HD en zone urbaine.</figcaption>
        </figure>

        <h3>M√©thodologie</h3>
        <ol>
            <li>Cr√©ation manuelle de v√©rit√©s-terrain en corrigeant la classification de dalles kilom√©triques pour disposer de r√©f√©rences fiables.</li>
            <li>D√©veloppement d‚Äôun score structurel calcul√© uniquement √† partir de la g√©om√©trie et de la r√©partition des classes (homog√©n√©it√©, coh√©rence locale, points isol√©s) pour rep√©rer automatiquement les cellules suspectes.</li>
            <li>Entra√Ænement de mod√®les de deep learning (architecture PointNet / PointNet++) pour apprendre √† d√©tecter, point par point, les erreurs de classification √† partir des dalles corrig√©es.</li>
        </ol>

        <h3>R√©sultats</h3>
        <p>
            Les r√©sultats montrent un constat nuanc√© : les m√©triques structurelles permettent de calculer un score de suspicion au sein de cellules dans une dalle LiDAR, produisant ainsi des cartes d‚Äôalerte utiles pour prioriser des contr√¥les humains.
        </p>

        <figure>
            <img src="images/lidar/carte_alerte.png" alt="Exemple d‚Äôune carte d‚Äôalerte en zone urbaine" class="project-image">
            <figcaption>Exemple d‚Äôune carte d‚Äôalerte en zone urbaine.</figcaption>
        </figure>

        <p>
            Pour chaque cellule :
            <ul>
                <li><strong>Score < 0.1 :</strong> Bonne classification (pas d'alerte)</li>
                <li><strong>0.4 > Score > 0.1 :</strong> Classification moyenne</li>
                <li><strong>Score > 0.4 :</strong> Classification mauvaise</li>
            </ul>
        </p>

        <p>
            Les mod√®les de deep learning fournissent des probabilit√©s de bonne classification interpr√©tables pour chaque classe.
            Cependant, leur performance reste limit√©e par plusieurs facteurs :
            <ul>
                <li>Le d√©s√©quilibre des classes.</li>
                <li>La diversit√© limit√©e des donn√©es d‚Äôentra√Ænement.</li>
                <li>Les contraintes mat√©rielles (m√©moire, calcul).</li>
            </ul>
        </p>

        <p>
            En pratique :
            <ul>
                <li>Les classes majoritaires sont bien reconnues.</li>
                <li>Les classes rares au sein d‚Äôune dalle sont encore mal d√©tect√©es.</li>
            </ul>
        </p>

        <p>
            Un exemple est donn√© ci-dessous avec les pr√©dictions du mod√®le sur la pr√©cision de 3 classes :
            <ul>
                <li><strong>‚Äúsol‚Äù</strong></li>
                <li><strong>‚Äúb√¢ti‚Äù</strong></li>
                <li><strong>‚Äúnon-class√©‚Äù</strong></li>
            </ul>
            dans 4 zones diff√©rentes.
        </p>

        <p>
            Les pr√©dictions du mod√®le correspondent √† des valeurs entre 0 et 1, que l‚Äôon peut traduire par :
            <blockquote>
                <em>‚Äúla probabilit√© moyenne qu‚Äôun point soit correctement class√© pour chaque classe.‚Äù</em>
            </blockquote>
            C‚Äôest une forme de niveau de confiance que l‚Äôon peut avoir dans la classification.
        </p>

        <figure>
            <img src="images/lidar/tableau.png" alt="Tableau r√©sumant les pr√©dictions du mod√®le d‚Äôapprentissage profond concernant la pr√©cision de 3 classes dans 4 zones diff√©rentes" style="width: 80%; height: auto; margin: 20px auto;">
            <figcaption>Tableau r√©sumant les pr√©dictions du mod√®le d‚Äôapprentissage profond concernant la pr√©cision de 3 classes dans 4 zones diff√©rentes.</figcaption>
        </figure>

        <p>
            En conclusion, une approche combin√©e, m√™lant m√©triques structurelles pour la d√©tection rapide et l‚Äôapprentissage supervis√© pour affiner les probabilit√©s, appara√Æt la plus prometteuse.
            Pour passer √† l‚Äô√©chelle, il faudra prioritairement enrichir les r√©f√©rences manuelles (annotations cibl√©es), diversifier les environnements d‚Äôentra√Ænement et int√©grer des pipelines reproductibles pour industrialiser le contr√¥le qualit√© LiDAR HD.
        </p>

        <div class="project-skills">
            <h3>Outils / donn√©es</h3>
            <ul>
                <li>Python</li>
                <li>Tensorflow / PyTorch</li>
                <li>PointNet++</li>
                <li>Terrasolid</li>
            </ul>
        </div>
    </section>






    <section class="project" id="project-segmentation">
        <h2>Segmentation s√©mantique de l‚Äôoccupation du sol par Deep Learning</h2>

        <p class="project-context">
            Projet de recherche portant sur la mise en ≈ìuvre d‚Äôun mod√®le de segmentation s√©mantique automatis√©e bas√© sur l‚Äôarchitecture <strong>DeepLabv3+</strong>, appliqu√© √† des images satellites Sentinel-2 pour la cartographie de l‚Äôoccupation du sol.
        </p>

        <h3>Objectifs du projet</h3>
        <p>
            L‚Äôobjectif principal de ce travail est de d√©velopper un mod√®le de segmentation s√©mantique capable de pr√©dire des cartes d‚Äôoccupation du sol √† partir d‚Äôimages Sentinel-2, en s‚Äôappuyant sur des donn√©es labellis√©es issues du jeu de donn√©es <strong>LandCoverNet</strong>.
        </p>

        <p>
            En parall√®le, le projet s‚Äôint√©resse aux d√©fis li√©s √† la <strong>labellisation manuelle</strong> de ces donn√©es, en √©valuant le temps n√©cessaire, la complexit√© et les limites de cette approche. L‚Äôobjectif final est de comparer la labellisation manuelle √† l‚Äôapprentissage profond, et d‚Äô√©valuer dans quelle mesure un mod√®le de deep learning peut am√©liorer la classification de l‚Äôoccupation du sol.
        </p>

        <p>
            Une comparaison avec un mod√®le <strong>U-Net</strong>, d√©velopp√© par un autre groupe de travail, permet √©galement de situer les performances de DeepLabv3+.
        </p>

        <h3>Donn√©es utilis√©es</h3>
        <ul>
            <li>Images multispectrales Sentinel-2</li>
            <li>Donn√©es labellis√©es LandCoverNet</li>
            <li>Annotations manuelles pour l‚Äô√©valuation du processus de labellisation</li>
        </ul>

        <h3>Annotation et labellisation</h3>
        <p>
            Une partie du projet est consacr√©e √† la labellisation manuelle des donn√©es, r√©alis√©e √† l‚Äôaide de l‚Äôoutil CVAT. Cette √©tape permet d‚Äô√©valuer la charge de travail, les difficult√©s rencontr√©es et la subjectivit√© associ√©e √† la production de v√©rit√©s terrain.
        </p>

        <figure>
            <img src="images/segmentation/cvat_classes2.png" alt="Annotation manuelle des images Sentinel-2 avec CVAT" style="width: 60%; height: auto; margin: 20px auto;">
            <figcaption>Annotation manuelle des images Sentinel-2 avec CVAT.</figcaption>
        </figure>

        <h3>M√©thodologie</h3>
        <ol>
            <li>Pr√©traitement des images Sentinel-2</li>
            <li>Analyse des donn√©es LandCoverNet</li>
            <li>Labellisation manuelle via CVAT</li>
            <li>Entra√Ænement du mod√®le DeepLabv3+</li>
            <li>√âvaluation des performances de segmentation</li>
            <li>Comparaison avec la labellisation manuelle et le mod√®le U-Net</li>
        </ol>

        <h3>R√©sultats de segmentation</h3>
        <p>
            Les cartes de segmentation produites par DeepLabv3+ montrent une meilleure coh√©rence spatiale et une capacit√© accrue √† g√©n√©raliser par rapport aux annotations manuelles, notamment sur les zones complexes.
        </p>

        <figure>
            <img src="images/segmentation/predictions.png" alt="Image Sentinel-2 et carte d‚Äôoccupation du sol de r√©f√©rence" style="width: 70%; height: auto; margin: 20px auto;">
            <figcaption>Comparaison entre une image Sentinel-2, la carte d‚Äôoccupation du sol de r√©f√©rence, et la carte d'occupation produite par DeepLabV3+.</figcaption>
        </figure>

        <h3>Comparaison des m√©thodes</h3>
        <p>
            Une comparaison qualitative et quantitative est r√©alis√©e entre la labellisation manuelle, le mod√®le DeepLabv3+ et un mod√®le U-Net. Les r√©sultats mettent en √©vidence les avantages du deep learning en termes de rapidit√©, de reproductibilit√© et de qualit√© de segmentation. Ils mettent √©galement en √©vidence les limites d'un mod√®le complexe (et plus robuste) comme DeepLabV3+ lorsqu'il manque encore de donn√©es d'entra√Ænement en quantit√© et diversit√©.
        </p>

        <figure>
            <img src="images/segmentation/comparaisons_modeles.png" alt="Comparaison entre labellisation manuelle, DeepLabv3+ et U-Net" style="margin: 20px auto;">
            <figcaption>Comparaison entre une image Sentinel-2, U-Net et DeepLabv3+.</figcaption>
        </figure>

        <h3>Conclusion</h3>
        <p>
            Ce projet a permis de d√©montrer l‚Äôint√©r√™t du deep learning pour la segmentation s√©mantique d‚Äôimages satellitaires, en particulier avec l‚Äôarchitecture DeepLabV3+. Le mod√®le est capable de produire des cartes d‚Äôoccupation du sol coh√©rentes √† partir d‚Äôimages Sentinel-2.
        </p>

        <p>
            La comparaison avec la labellisation manuelle met en √©vidence un compromis clair : si l‚Äôannotation humaine reste plus pr√©cise, elle est tr√®s co√ªteuse en temps, tandis que la segmentation automatique permet un gain de productivit√© significatif.
        </p>

        <p>
            Ces r√©sultats confirment le potentiel de l‚Äôintelligence artificielle pour l‚Äôanalyse environnementale √† grande √©chelle, tout en soulignant l‚Äôimportance de la qualit√© des donn√©es et de l‚Äôoptimisation des param√®tres pour am√©liorer les performances.
        </p>

        <div class="project-skills">
            <h3>Outils / donn√©es</h3>
            <ul>
                <li>Python</li>
                <li>Deep Learning</li>
                <li>Segmentation s√©mantique</li>
                <li>DeepLabv3+</li>
                <li>U-Net</li>
                <li>Sentinel-2</li>
                <li>LandCoverNet</li>
                <li>CVAT</li>
                <li>SIG</li>
            </ul>
        </div>
    </section>






    <section class="project" id="project-sig">
        <h2>Distribution spatiale des odonymes f√©minins dans la ville de Paris</h2>
        <p>
            Ce projet, r√©alis√© en collaboration avec Benjamin Lecordix et Johann Palos, vise √† analyser la distribution spatiale des odonymes (noms des rues) f√©minins √† Paris.
            Commandit√© par Alexandra Mallah (doctorante au laboratoire G√©ographie-cit√©s), ce projet s'inscrit dans le cadre d'une th√®se sur la repr√©sentation des genres dans l'espace urbain.
        </p>

        <h3>Contexte</h3>
        <p>
            Les noms de voie ne sont pas de simples rep√®res g√©ographiques, mais de v√©ritables marqueurs identitaires qui inscrivent dans l‚Äôespace quotidien une version autoris√©e de l‚Äôhistoire.
            √Ä Paris, les odonymes f√©minins restent sous-repr√©sent√©s, refl√©tant des in√©galit√©s de genre dans la m√©moire collective.
        </p>

        <figure>
            <img src="images/odonymes/image_presentation_panneau.jpg" alt="Plaque de rue Gis√®le Halimi" style="width: 60%; height: auto; margin: 20px auto;">
            <figcaption>Panneau de rue honorant Gis√®le Halimi, avocate et femme politique.</figcaption>
        </figure>

        <h3>M√©thode</h3>
        <p>
            Nous avons utilis√© des donn√©es de la ville de Paris pour g√©or√©f√©rencer les odonymes et analyser leur r√©partition en fonction de trois types de centralit√©s :
            <ul>
                <li>Centralit√© touristique</li>
                <li>Centralit√© politique</li>
                <li>Centralit√© de fr√©quentation p√©destre</li>
            </ul>
        </p>
        <p>
            Nous avons cr√©√© des zones tampons autour des sites touristiques et politiques, puis analys√© la proportion de surface des odonymes f√©minins dans ces zones.
        </p>

        <h3>R√©sultats</h3>
        <p>
            Les r√©sultats montrent que les odonymes f√©minins sont sous-repr√©sent√©s dans les zones centrales et les grandes art√®res.
            Malgr√© une politique de f√©minisation des odonymes initi√©e apr√®s 2001, les odonymes f√©minins restent majoritairement situ√©s en p√©riph√©rie ou dans des espaces secondaires.
        </p>

        <figure>
            <img src="images/odonymes/carte_finale_cto.png" alt="Carte des centralit√©s touristiques" style="width: 80%; height: auto; margin: 20px auto;">
            <figcaption>Carte des centralit√©s touristiques et r√©partition des odonymes f√©minins.</figcaption>
        </figure>

        <figure>
            <img src="images/odonymes/carte_finale_cpo.png" alt="Carte des centralit√©s politiques" style="width: 80%; height: auto; margin: 20px auto;">
            <figcaption>Carte des centralit√©s politiques et r√©partition des odonymes f√©minins.</figcaption>
        </figure>

        <figure>
            <img src="images/odonymes/carte_freq_niv.png" alt="Carte de la fr√©quentation p√©destre" style="width: 80%; height: auto; margin: 20px auto;">
            <figcaption>Carte de la fr√©quentation p√©destre et r√©partition des odonymes f√©minins.</figcaption>
        </figure>

        <figure>
            <img src="images/odonymes/tableau_freq.png" alt="Tableau fr√©quentation p√©destre" style="width: 70%; height: auto; margin: 20px auto;">
            <figcaption>Tableau des proportions de rues (en longueur) seln le genre.</figcaption>
        </figure>

        <h3>Conclusion</h3>
        <p>
            Cette √©tude met en √©vidence des disparit√©s significatives dans la repr√©sentation des figures f√©minines dans l'espace public parisien.
            Malgr√© une augmentation du nombre d'odonymes f√©minins apr√®s la loi de 2001, leur localisation et leur typologie montrent que des in√©galit√©s persistent.
        </p>

        <div class="project-skills">
            <h3>Comp√©tences mobilis√©es</h3>
            <ul>
                <li>Traitement des donn√©es</li>
                <li>Analyse spatiale</li>
                <li>G√©or√©f√©rencement</li>
                <li>QGIS</li>
                <li>Python</li>
                <li>Distance de Levenshtein</li>
                <li>Zones tampons</li>
                <li>Analyse statistique</li>
                <li>OpenData Paris</li>
                <li>Visualisation cartographique</li>
            </ul>
        </div>
    </section>

    <section class="project" id="project-ndwi">
        <h2>D√©tection de l‚Äôeau par NDWI</h2>

        <p class="project-context">
            Mini-projet visant √† d√©tecter automatiquement les surfaces en eau √† partir d‚Äôimages Sentinel-2 en utilisant l‚Äôindice NDWI, sans recours au Deep Learning.
        </p>

        <h3>Contexte & objectif</h3>
        <p>
            L‚Äôobjectif de ce projet √©tait de d√©velopper une m√©thode simple, robuste et interpr√©table pour la d√©tection de l‚Äôeau √† partir d‚Äôimages satellites, en s‚Äôappuyant sur des indices spectraux classiques et une analyse spatiale des erreurs.
        </p>

        <h3>Donn√©es</h3>
        <ul>
            <li>Sentinel-2 (bandes verte et infrarouge)</li>
            <li>Donn√©es de r√©f√©rence JRC Global Surface Water</li>
        </ul>

        <h3>M√©thodologie</h3>
        <ol>
            <li>Calcul du NDWI</li>
            <li>Seuillage pour la classification eau / non-eau</li>
            <li>Comparaison avec les donn√©es de r√©f√©rence JRC</li>
            <li>Analyse spatiale des erreurs de classification</li>
        </ol>

        <h3>R√©sultats</h3>
        <p>
            Les r√©sultats montrent une bonne d√©tection globale des surfaces en eau, avec des erreurs localis√©es principalement en zones urbaines et ombrag√©es.
        </p>

        <figure>
            <img src="images/carte_finale.png" alt="Carte des erreurs de d√©tection d'eau" class="project-image">
            <figcaption>Carte des erreurs de d√©tection d'eau.</figcaption>
        </figure>

        <div class="project-skills">
            <span>Sentinel-2</span>
            <span>NDWI</span>
            <span>Python</span>
            <span>Analyse spatiale</span>
            <span>SIG</span>
        </div>
    </section>
</main>

 <footer>
            <p>&copy; 2026 Eliot Barri√®re. Tous droits r√©serv√©s.</p>
        </footer>
    </div>
    <script src="script.js"></script>
</body>
</html>

